{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset Analysis: PCA + K-Means Clustering\n",
    "\n",
    "This notebook performs a comprehensive analysis of the MNIST dataset using:\n",
    "- **Principal Component Analysis (PCA)** for dimensionality reduction\n",
    "- **K-Means Clustering** for unsupervised classification\n",
    "\n",
    "We'll explore how different numbers of principal components affect clustering performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    adjusted_rand_score,\n",
    "    normalized_mutual_info_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path('results')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "print(f\"✓ Results directory created: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare MNIST Dataset\n",
    "\n",
    "We'll load a subset of 10,000 samples from the MNIST dataset to make computations manageable while still having enough data for meaningful analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "print(\"Loading MNIST dataset...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "X, y = mnist.data, mnist.target\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X, dtype=np.float32)\n",
    "y = np.array(y, dtype=np.int32)\n",
    "\n",
    "# Select random subset of 10,000 samples\n",
    "n_samples = 10000\n",
    "np.random.seed(RANDOM_STATE)\n",
    "indices = np.random.choice(len(X), n_samples, replace=False)\n",
    "X_subset = X[indices]\n",
    "y_subset = y[indices]\n",
    "\n",
    "print(f\"\\n✓ Dataset loaded successfully\")\n",
    "print(f\"  - Total samples: {len(X_subset):,}\")\n",
    "print(f\"  - Features per sample: {X_subset.shape[1]:,}\")\n",
    "print(f\"  - Classes: {np.unique(y_subset)}\")\n",
    "print(f\"  - Data shape: {X_subset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Normalization and Standardization\n",
    "\n",
    "We'll normalize pixel values to [0, 1] range and then standardize for PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to [0, 1] range\n",
    "X_normalized = X_subset / 255.0\n",
    "\n",
    "# Standardize the data (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_normalized)\n",
    "\n",
    "print(\"✓ Data normalized and standardized\")\n",
    "print(f\"  - Mean: {X_scaled.mean():.6f}\")\n",
    "print(f\"  - Std: {X_scaled.std():.6f}\")\n",
    "print(f\"  - Min: {X_scaled.min():.6f}\")\n",
    "print(f\"  - Max: {X_scaled.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PCA Analysis - Variance Explained\n",
    "\n",
    "Let's first analyze how many components we need to capture most of the variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA with all components to analyze variance\n",
    "print(\"Performing PCA analysis...\")\n",
    "pca_full = PCA(random_state=RANDOM_STATE)\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# Calculate cumulative variance explained\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Find number of components for different variance thresholds\n",
    "n_components_80 = np.argmax(cumulative_variance >= 0.80) + 1\n",
    "n_components_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"\\n✓ PCA analysis complete\")\n",
    "print(f\"  - Components for 80% variance: {n_components_80}\")\n",
    "print(f\"  - Components for 90% variance: {n_components_90}\")\n",
    "print(f\"  - Components for 95% variance: {n_components_95}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Variance Explained\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Individual variance explained\n",
    "ax1.bar(range(1, 51), pca_full.explained_variance_ratio_[:50], alpha=0.7, color='steelblue')\n",
    "ax1.set_xlabel('Principal Component', fontsize=12)\n",
    "ax1.set_ylabel('Variance Explained Ratio', fontsize=12)\n",
    "ax1.set_title('Individual Variance Explained by First 50 Components', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative variance explained\n",
    "ax2.plot(range(1, 101), cumulative_variance[:100], linewidth=2.5, color='darkblue')\n",
    "ax2.axhline(y=0.80, color='green', linestyle='--', linewidth=2, label=f'80% ({n_components_80} components)')\n",
    "ax2.axhline(y=0.90, color='orange', linestyle='--', linewidth=2, label=f'90% ({n_components_90} components)')\n",
    "ax2.axhline(y=0.95, color='red', linestyle='--', linewidth=2, label=f'95% ({n_components_95} components)')\n",
    "ax2.set_xlabel('Number of Components', fontsize=12)\n",
    "ax2.set_ylabel('Cumulative Variance Explained', fontsize=12)\n",
    "ax2.set_title('Cumulative Variance Explained', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'variance_explained.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: variance_explained.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply PCA with Different Numbers of Components\n",
    "\n",
    "We'll apply PCA with 2, 10, and 30 components to compare their effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA with different numbers of components\n",
    "n_components_list = [2, 10, 30]\n",
    "pca_results = {}\n",
    "\n",
    "for n_comp in n_components_list:\n",
    "    print(f\"\\nApplying PCA with {n_comp} components...\")\n",
    "    pca = PCA(n_components=n_comp, random_state=RANDOM_STATE)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    variance_retained = pca.explained_variance_ratio_.sum()\n",
    "    \n",
    "    pca_results[n_comp] = {\n",
    "        'pca': pca,\n",
    "        'X_transformed': X_pca,\n",
    "        'variance_retained': variance_retained\n",
    "    }\n",
    "    \n",
    "    print(f\"  ✓ Variance retained: {variance_retained:.4f} ({variance_retained*100:.2f}%)\")\n",
    "    print(f\"  ✓ Transformed shape: {X_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. K-Means Clustering\n",
    "\n",
    "Apply K-Means clustering (k=10) to each PCA-transformed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means clustering for each PCA configuration\n",
    "n_clusters = 10\n",
    "clustering_results = {}\n",
    "\n",
    "for n_comp in n_components_list:\n",
    "    print(f\"\\nApplying K-Means (k={n_clusters}) on {n_comp} components...\")\n",
    "    X_pca = pca_results[n_comp]['X_transformed']\n",
    "    \n",
    "    # Fit K-Means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_STATE, n_init=10, max_iter=300)\n",
    "    labels_pred = kmeans.fit_predict(X_pca)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    silhouette = silhouette_score(X_pca, labels_pred)\n",
    "    ari = adjusted_rand_score(y_subset, labels_pred)\n",
    "    nmi = normalized_mutual_info_score(y_subset, labels_pred)\n",
    "    \n",
    "    clustering_results[n_comp] = {\n",
    "        'kmeans': kmeans,\n",
    "        'labels': labels_pred,\n",
    "        'silhouette': silhouette,\n",
    "        'ari': ari,\n",
    "        'nmi': nmi\n",
    "    }\n",
    "    \n",
    "    print(f\"  ✓ Silhouette Score: {silhouette:.4f}\")\n",
    "    print(f\"  ✓ Adjusted Rand Index: {ari:.4f}\")\n",
    "    print(f\"  ✓ Normalized Mutual Information: {nmi:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Summary\n",
    "\n",
    "Let's create a comprehensive table with all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary table\n",
    "results_summary = []\n",
    "\n",
    "for n_comp in n_components_list:\n",
    "    results_summary.append({\n",
    "        'Components': n_comp,\n",
    "        'Variance Retained': f\"{pca_results[n_comp]['variance_retained']:.4f}\",\n",
    "        'Silhouette Score': f\"{clustering_results[n_comp]['silhouette']:.4f}\",\n",
    "        'ARI': f\"{clustering_results[n_comp]['ari']:.4f}\",\n",
    "        'NMI': f\"{clustering_results[n_comp]['nmi']:.4f}\"\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLUSTERING PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization: 2D Cluster Plots\n",
    "\n",
    "Visualize the clustering results using the 2-component PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: 2D Clusters - Predicted Labels\n",
    "X_2d = pca_results[2]['X_transformed']\n",
    "labels_2d = clustering_results[2]['labels']\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=labels_2d, cmap='tab10', \n",
    "                     alpha=0.6, s=30, edgecolors='k', linewidth=0.5)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.xlabel('First Principal Component', fontsize=12)\n",
    "plt.ylabel('Second Principal Component', fontsize=12)\n",
    "plt.title('K-Means Clustering Results (2 Components) - Predicted Labels', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'clusters_2d_predicted.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: clusters_2d_predicted.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: 2D Clusters - True Labels\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_subset, cmap='tab10', \n",
    "                     alpha=0.6, s=30, edgecolors='k', linewidth=0.5)\n",
    "plt.colorbar(scatter, label='Digit')\n",
    "plt.xlabel('First Principal Component', fontsize=12)\n",
    "plt.ylabel('Second Principal Component', fontsize=12)\n",
    "plt.title('K-Means Clustering Results (2 Components) - True Labels', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'clusters_2d_true.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: clusters_2d_true.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrices\n",
    "\n",
    "Compare predicted clusters with true labels for each PCA configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 4: Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for idx, n_comp in enumerate(n_components_list):\n",
    "    labels_pred = clustering_results[n_comp]['labels']\n",
    "    cm = confusion_matrix(y_subset, labels_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], \n",
    "                cbar_kws={'label': 'Count'}, square=True)\n",
    "    axes[idx].set_xlabel('Predicted Cluster', fontsize=11)\n",
    "    axes[idx].set_ylabel('True Digit', fontsize=11)\n",
    "    axes[idx].set_title(f'Confusion Matrix\\n({n_comp} Components)', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Metrics Comparison\n",
    "\n",
    "Visual comparison of all performance metrics across different PCA configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 5: Metrics Comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Prepare data\n",
    "components = n_components_list\n",
    "silhouette_scores = [clustering_results[n]['silhouette'] for n in components]\n",
    "ari_scores = [clustering_results[n]['ari'] for n in components]\n",
    "nmi_scores = [clustering_results[n]['nmi'] for n in components]\n",
    "variance_retained = [pca_results[n]['variance_retained'] for n in components]\n",
    "\n",
    "# Silhouette Score\n",
    "axes[0, 0].plot(components, silhouette_scores, marker='o', linewidth=2.5, \n",
    "                markersize=10, color='steelblue')\n",
    "axes[0, 0].set_xlabel('Number of Components', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Silhouette Score', fontsize=11)\n",
    "axes[0, 0].set_title('Silhouette Score vs Components', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xticks(components)\n",
    "\n",
    "# Adjusted Rand Index\n",
    "axes[0, 1].plot(components, ari_scores, marker='s', linewidth=2.5, \n",
    "                markersize=10, color='forestgreen')\n",
    "axes[0, 1].set_xlabel('Number of Components', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Adjusted Rand Index', fontsize=11)\n",
    "axes[0, 1].set_title('ARI vs Components', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_xticks(components)\n",
    "\n",
    "# Normalized Mutual Information\n",
    "axes[1, 0].plot(components, nmi_scores, marker='^', linewidth=2.5, \n",
    "                markersize=10, color='darkorange')\n",
    "axes[1, 0].set_xlabel('Number of Components', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Normalized Mutual Information', fontsize=11)\n",
    "axes[1, 0].set_title('NMI vs Components', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_xticks(components)\n",
    "\n",
    "# Combined metrics\n",
    "x = np.arange(len(components))\n",
    "width = 0.25\n",
    "axes[1, 1].bar(x - width, silhouette_scores, width, label='Silhouette', alpha=0.8)\n",
    "axes[1, 1].bar(x, ari_scores, width, label='ARI', alpha=0.8)\n",
    "axes[1, 1].bar(x + width, nmi_scores, width, label='NMI', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Number of Components', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Score', fontsize=11)\n",
    "axes[1, 1].set_title('All Metrics Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(components)\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: metrics_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sample Digits from Each Cluster\n",
    "\n",
    "Visualize sample digits from each cluster to understand what the algorithm learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 6: Sample digits from each cluster (30 components)\n",
    "labels_30 = clustering_results[30]['labels']\n",
    "n_samples_per_cluster = 10\n",
    "\n",
    "fig, axes = plt.subplots(n_clusters, n_samples_per_cluster, figsize=(20, 20))\n",
    "fig.suptitle('Sample Digits from Each Cluster (30 Components)', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "    cluster_indices = np.where(labels_30 == cluster)[0]\n",
    "    sample_indices = np.random.choice(cluster_indices, \n",
    "                                     min(n_samples_per_cluster, len(cluster_indices)), \n",
    "                                     replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        axes[cluster, i].imshow(X_subset[idx].reshape(28, 28), cmap='gray')\n",
    "        axes[cluster, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[cluster, i].set_title(f'Cluster {cluster}', \n",
    "                                       fontsize=11, fontweight='bold', loc='left')\n",
    "        # Show true label\n",
    "        axes[cluster, i].text(0.5, -0.1, f'True: {y_subset[idx]}', \n",
    "                             transform=axes[cluster, i].transAxes,\n",
    "                             ha='center', fontsize=9, color='blue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'cluster_samples.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: cluster_samples.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Elbow Method Analysis\n",
    "\n",
    "Determine the optimal number of clusters using the elbow method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 7: Elbow Method\n",
    "print(\"Performing elbow method analysis...\")\n",
    "\n",
    "# Use 30 components for this analysis\n",
    "X_30 = pca_results[30]['X_transformed']\n",
    "k_range = range(2, 21)\n",
    "inertias = []\n",
    "silhouette_scores_elbow = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
    "    kmeans.fit(X_30)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores_elbow.append(silhouette_score(X_30, kmeans.labels_))\n",
    "    print(f\"  k={k:2d}: Inertia={kmeans.inertia_:.2f}, Silhouette={silhouette_scores_elbow[-1]:.4f}\")\n",
    "\n",
    "# Plot elbow method results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Inertia plot\n",
    "ax1.plot(k_range, inertias, marker='o', linewidth=2.5, markersize=8, color='darkblue')\n",
    "ax1.axvline(x=10, color='red', linestyle='--', linewidth=2, label='k=10 (MNIST digits)')\n",
    "ax1.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "ax1.set_ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12)\n",
    "ax1.set_title('Elbow Method - Inertia', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette score plot\n",
    "ax2.plot(k_range, silhouette_scores_elbow, marker='s', linewidth=2.5, \n",
    "         markersize=8, color='darkgreen')\n",
    "ax2.axvline(x=10, color='red', linestyle='--', linewidth=2, label='k=10 (MNIST digits)')\n",
    "ax2.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "ax2.set_ylabel('Silhouette Score', fontsize=12)\n",
    "ax2.set_title('Elbow Method - Silhouette Score', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'elbow_method.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Saved: elbow_method.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Analysis and Interpretation\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "#### 1. **Variance Retention**\n",
    "- 2 components retain only a small fraction of variance but provide good visualization\n",
    "- 10 components capture significantly more variance\n",
    "- 30 components retain most of the important information\n",
    "\n",
    "#### 2. **Clustering Performance**\n",
    "- **Silhouette Score**: Measures how well-separated the clusters are\n",
    "  - Higher is better (range: -1 to 1)\n",
    "  - Generally improves with more components as clusters become more distinct\n",
    "  \n",
    "- **Adjusted Rand Index (ARI)**: Measures similarity between predicted and true labels\n",
    "  - Range: -1 to 1 (1 = perfect match)\n",
    "  - Shows how well K-Means recovers the true digit structure\n",
    "  \n",
    "- **Normalized Mutual Information (NMI)**: Measures information shared between clusterings\n",
    "  - Range: 0 to 1 (1 = perfect agreement)\n",
    "  - Indicates the quality of unsupervised learning\n",
    "\n",
    "#### 3. **Dimensionality Trade-offs**\n",
    "- **2 Components**: Best for visualization, but loses too much information for accurate clustering\n",
    "- **10 Components**: Good balance between dimensionality reduction and clustering performance\n",
    "- **30 Components**: Best clustering performance, retains most discriminative features\n",
    "\n",
    "#### 4. **Elbow Method Insights**\n",
    "- The elbow method helps identify the optimal number of clusters\n",
    "- For MNIST, k=10 is natural (10 digits), but the elbow might suggest different values\n",
    "- This demonstrates the difference between unsupervised and supervised perspectives\n",
    "\n",
    "### Conclusions:\n",
    "\n",
    "1. **PCA is effective** for dimensionality reduction in high-dimensional image data\n",
    "2. **More components generally improve clustering** but with diminishing returns\n",
    "3. **K-Means can discover digit structure** without labels (unsupervised learning)\n",
    "4. **There's a trade-off** between computational efficiency and clustering quality\n",
    "5. The **confusion matrices** reveal which digits are commonly confused by the algorithm\n",
    "\n",
    "### Practical Implications:\n",
    "\n",
    "- For **visualization**: Use 2-3 components\n",
    "- For **clustering**: Use 20-50 components depending on computational resources\n",
    "- For **classification**: Consider supervised methods or more components\n",
    "- The **choice of k** and number of components depends on the specific application and requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Final Summary\n",
    "\n",
    "All analysis complete! Generated visualizations saved to the `results/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nGenerated Visualizations:\")\n",
    "print(\"  1. ✓ variance_explained.png - PCA variance analysis\")\n",
    "print(\"  2. ✓ clusters_2d_predicted.png - 2D clustering with predicted labels\")\n",
    "print(\"  3. ✓ clusters_2d_true.png - 2D clustering with true labels\")\n",
    "print(\"  4. ✓ confusion_matrices.png - Confusion matrices for all configurations\")\n",
    "print(\"  5. ✓ metrics_comparison.png - Performance metrics comparison\")\n",
    "print(\"  6. ✓ cluster_samples.png - Sample digits from each cluster\")\n",
    "print(\"  7. ✓ elbow_method.png - Optimal cluster number analysis\")\n",
    "print(\"\\nAll files saved to: results/\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Display final metrics table\n",
    "print(\"\\nFinal Performance Metrics:\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
